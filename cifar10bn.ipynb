{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cifar\n",
    "tf.logging.set_verbosity(tf.logging.WARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and extract the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar.prepare_cifar_10()\n",
    "cifar10_labels = cifar.cifar10_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model function\n",
    "Here we have defined a common model function which runs for both batch_normalized and non-batch_normalized networks.\n",
    "\n",
    "#### Batch Norm before or after activation\n",
    "This is a long-standing debate about Batch Normalization. To put batch norm before or after activation. I have chosen to put it after the activation. And more specifically, at the input of each layer. This makes more sense, since batch norm is introduced to reduce the covariant shift in input of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(features, labels, mode, params):\n",
    "    layer = features['images']\n",
    "    if params[\"print_shapes\"]:\n",
    "        print(layer.shape)\n",
    "\n",
    "    for filt, kern, stride in zip(params['filters'], params['kern'], params['strides']):\n",
    "        if params['with_bn']:\n",
    "            layer = tf.layers.batch_normalization(\n",
    "                layer, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "        layer = tf.layers.conv2d(\n",
    "            layer, filt, kern, stride, activation=tf.nn.relu)\n",
    "        if params[\"print_shapes\"]: \n",
    "            print(layer.shape)\n",
    "\n",
    "    layer = tf.layers.flatten(layer)\n",
    "    if params[\"print_shapes\"]:\n",
    "        print(layer.shape)\n",
    "\n",
    "    for units in params['dense']:\n",
    "        if params['with_bn']:\n",
    "            layer = tf.layers.batch_normalization(\n",
    "                layer, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "        layer = tf.layers.dense(\n",
    "            layer, units, activation=tf.nn.relu)\n",
    "        if params[\"print_shapes\"]:\n",
    "            print(layer.shape)\n",
    "\n",
    "    if params['with_bn']:\n",
    "        layer = tf.layers.batch_normalization(\n",
    "            layer, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    logits = tf.layers.dense(layer, 10)\n",
    "    if params[\"print_shapes\"]:\n",
    "        print(logits.shape)\n",
    "    cls = tf.argmax(logits, -1)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions={\n",
    "            \"class\": cls,\n",
    "            \"score\": tf.nn.softmax(logits)\n",
    "        })\n",
    "\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels, logits)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops={\n",
    "            \"accuracy\": tf.metrics.accuracy(labels, cls)\n",
    "        })\n",
    "\n",
    "    opt = tf.train.AdamOptimizer().minimize(\n",
    "        loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inp_fn():\n",
    "    return tf.data.Dataset.from_generator(cifar.cifar10_train,({\"images\":tf.float32},tf.int64),\n",
    "                                         ({\"images\":tf.TensorShape([None,32,32,3])},tf.TensorShape(None)))\n",
    "\n",
    "def test_inp_fn():\n",
    "    return tf.data.Dataset.from_generator(cifar.cifar10_test,({\"images\":tf.float32},tf.int64),\n",
    "                                         ({\"images\":tf.TensorShape([None,32,32,3])},tf.TensorShape(None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without BatchNorm (no dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wobn_params = {\n",
    "    \"filters\": [30, 50,60],\n",
    "    \"kern\": [[3, 3]]*3,\n",
    "    \"strides\": [[2,2],[1,1],[1,1]],\n",
    "    \"dense\": [3500,700],\n",
    "    \"with_bn\": False,\n",
    "    \"print_shapes\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View layer shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 32, 32, 3)\n",
      "(10, 15, 15, 30)\n",
      "(10, 13, 13, 50)\n",
      "(10, 11, 11, 60)\n",
      "(10, 7260)\n",
      "(10, 3500)\n",
      "(10, 700)\n",
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "model({\"images\":tf.placeholder(tf.float32,(10,32,32,3))},\n",
    "      tf.placeholder(tf.int32,(10)),tf.estimator.ModeKeys.TRAIN,wobn_params)\n",
    "wobn_params[\"print_shapes\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wobn = tf.estimator.Estimator(model, 'wobn-ckpts', config=tf.estimator.RunConfig(save_summary_steps=2),\n",
    "                              params=wobn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run tensorboard with:\n",
    "```\n",
    "tensorboard --logdir wobn-ckpts\n",
    "```\n",
    "in current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.3379, 'loss': 1.8762674, 'global_step': 20}\n",
      "{'accuracy': 0.4551, 'loss': 1.4975078, 'global_step': 40}\n",
      "{'accuracy': 0.5002, 'loss': 1.3790355, 'global_step': 60}\n",
      "{'accuracy': 0.5239, 'loss': 1.3139155, 'global_step': 80}\n",
      "{'accuracy': 0.5495, 'loss': 1.2388148, 'global_step': 100}\n",
      "{'accuracy': 0.5889, 'loss': 1.1540627, 'global_step': 120}\n",
      "{'accuracy': 0.5812, 'loss': 1.1657984, 'global_step': 140}\n",
      "{'accuracy': 0.6214, 'loss': 1.0684103, 'global_step': 160}\n",
      "{'accuracy': 0.6337, 'loss': 1.0501449, 'global_step': 180}\n",
      "{'accuracy': 0.6292, 'loss': 1.0418706, 'global_step': 200}\n",
      "{'accuracy': 0.6272, 'loss': 1.0928996, 'global_step': 220}\n",
      "{'accuracy': 0.6459, 'loss': 1.0434248, 'global_step': 240}\n",
      "{'accuracy': 0.6463, 'loss': 1.0748448, 'global_step': 260}\n",
      "{'accuracy': 0.6411, 'loss': 1.119929, 'global_step': 280}\n",
      "{'accuracy': 0.6517, 'loss': 1.1203771, 'global_step': 300}\n",
      "{'accuracy': 0.6285, 'loss': 1.2333841, 'global_step': 320}\n",
      "{'accuracy': 0.6551, 'loss': 1.1896093, 'global_step': 340}\n",
      "{'accuracy': 0.6059, 'loss': 1.4606433, 'global_step': 360}\n",
      "{'accuracy': 0.6492, 'loss': 1.2470007, 'global_step': 380}\n",
      "{'accuracy': 0.6122, 'loss': 1.4908367, 'global_step': 400}\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    wobn.train(inp_fn)\n",
    "    print(wobn.evaluate(test_inp_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wbn_params = {\n",
    "    \"filters\": [30, 50,60],\n",
    "    \"kern\": [[3, 3]]*3,\n",
    "    \"strides\": [[2,2],[1,1],[1,1]],\n",
    "    \"dense\": [3500,700],\n",
    "    \"with_bn\": True,\n",
    "    \"print_shapes\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View layer shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 32, 32, 3)\n",
      "(10, 15, 15, 30)\n",
      "(10, 13, 13, 50)\n",
      "(10, 11, 11, 60)\n",
      "(10, 7260)\n",
      "(10, 3500)\n",
      "(10, 700)\n",
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "model({\"images\":tf.placeholder(tf.float32,(10,32,32,3))},\n",
    "      tf.placeholder(tf.int32,(10)),tf.estimator.ModeKeys.TRAIN,wbn_params)\n",
    "wbn_params[\"print_shapes\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wbn = tf.estimator.Estimator(model, 'wbn-ckpts', config=tf.estimator.RunConfig(save_summary_steps=2),\n",
    "                              params=wbn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run tensorboard with:\n",
    "```\n",
    "tensorboard --logdir wbn-ckpts --port 6007\n",
    "```\n",
    "in current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.1005, 'loss': 2.34123, 'global_step': 20}\n",
      "{'accuracy': 0.1, 'loss': 2.3317132, 'global_step': 40}\n",
      "{'accuracy': 0.1, 'loss': 2.3635697, 'global_step': 60}\n",
      "{'accuracy': 0.1, 'loss': 2.3704066, 'global_step': 80}\n",
      "{'accuracy': 0.1, 'loss': 2.3658943, 'global_step': 100}\n"
     ]
    }
   ],
   "source": [
    "for i in range(5): \n",
    "    wbn.train(inp_fn)\n",
    "    print(wbn.evaluate(test_inp_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "#### Without BN\n",
    "![Without BN](images/wobn.png)\n",
    "#### With BN\n",
    "![With BN](images/wbn.png)\n",
    "\n",
    "As we can see, the loss converges much faster in the version using Batch Normalization. Version without BN takes about 370 steps to reach 0.2 loss, whereas version with BN does it in just 75. Also, we can see that the loss reduction is gradual. There are no ups and downs. This is due to the reduction of internal covariant shift for each layer as done by BN.\n",
    "\n",
    "#### Overfitting\n",
    "Here, we can see that the eval losses goes up during training - an obvious indication of overfitting. For now, we have concentrated only to know how the BN fastens up the regular networks. So, I didn't use any regularization.\n",
    "\n",
    "BN also provides a weak form of regularization. But as we can see, it cannot prevent overfitting. It should used with any other regularization layers used normally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about Batch Normalization in the [original paper](https://arxiv.org/abs/1502.03167). You can try running this notebook in your local system or in [Google Colab](https://colab.research.google.com)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
