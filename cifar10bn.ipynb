{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cifar\n",
    "tf.logging.set_verbosity(tf.logging.WARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and extract the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar.prepare_cifar_10()\n",
    "cifar10_labels = cifar.cifar10_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model function\n",
    "Here we have defined a common model function which runs for both batchNormalized and non-batchNormalized networks.\n",
    "\n",
    "#### Batch Norm before or after activation\n",
    "This is a long-standing debate about Batch Normalization. To put batch norm before or after activation. I have chosen to put it after the activation. And more specifically, at the input of each layer. This makes more sense, since batch norm is introduced to reduce the covariant shift in input of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(features, labels, mode, params):\n",
    "    layer = features['images']\n",
    "    if params[\"print_shapes\"]:\n",
    "        print(layer.shape)\n",
    "\n",
    "    for filt, kern, stride in zip(params['filters'], params['kern'], params['strides']):\n",
    "        if params['with_bn']:\n",
    "            layer = tf.layers.batch_normalization(\n",
    "                layer, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "        layer = tf.layers.conv2d(\n",
    "            layer, filt, kern, stride, activation=tf.nn.relu)\n",
    "        if params[\"print_shapes\"]:\n",
    "            print(layer.shape)\n",
    "\n",
    "    layer = tf.layers.flatten(layer)\n",
    "    if params[\"print_shapes\"]:\n",
    "        print(layer.shape)\n",
    "\n",
    "    for units in params['dense']:\n",
    "        if params['with_bn']:\n",
    "            layer = tf.layers.batch_normalization(\n",
    "                layer, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "        layer = tf.layers.dense(\n",
    "            layer, units, activation=tf.nn.relu)\n",
    "        if params[\"print_shapes\"]:\n",
    "            print(layer.shape)\n",
    "\n",
    "    if params['with_bn']:\n",
    "        layer = tf.layers.batch_normalization(\n",
    "            layer, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    logits = tf.layers.dense(layer, 10)\n",
    "    if params[\"print_shapes\"]:\n",
    "        print(logits.shape)\n",
    "    cls = tf.argmax(logits, -1)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions={\n",
    "            \"class\": cls,\n",
    "            \"score\": tf.nn.softmax(logits)\n",
    "        })\n",
    "\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels, logits)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops={\n",
    "            \"accuracy\": tf.metrics.accuracy(labels, cls)\n",
    "        })\n",
    "    adam = tf.train.AdamOptimizer()\n",
    "    \n",
    "    if params['with_bn']:\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            opt = adam.minimize(loss,global_step=tf.train.get_global_step())\n",
    "    else:\n",
    "        opt = adam.minimize(loss,global_step=tf.train.get_global_step())\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inp_fn():\n",
    "    return tf.data.Dataset.from_generator(cifar.cifar10_train, ({\"images\": tf.float32}, tf.int64),\n",
    "                                          ({\"images\": tf.TensorShape([None, 32, 32, 3])}, tf.TensorShape(None)))\n",
    "\n",
    "\n",
    "def test_inp_fn():\n",
    "    return tf.data.Dataset.from_generator(cifar.cifar10_test, ({\"images\": tf.float32}, tf.int64),\n",
    "                                          ({\"images\": tf.TensorShape([None, 32, 32, 3])}, tf.TensorShape(None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"filters\": [30, 50, 60],\n",
    "    \"kern\": [[3, 3]]*3,\n",
    "    \"strides\": [[2, 2], [1, 1], [1, 1]],\n",
    "    \"dense\": [3500, 700],\n",
    "    \"with_bn\": False,\n",
    "    \"print_shapes\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View layer shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 32, 32, 3)\n",
      "(10, 15, 15, 30)\n",
      "(10, 13, 13, 50)\n",
      "(10, 11, 11, 60)\n",
      "(10, 7260)\n",
      "(10, 3500)\n",
      "(10, 700)\n",
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "model({\"images\": tf.placeholder(tf.float32, (10, 32, 32, 3))},\n",
    "      tf.placeholder(tf.int32, (10)), tf.estimator.ModeKeys.TRAIN, hparams)\n",
    "hparams[\"print_shapes\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without BatchNorm (no dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wobn = tf.estimator.Estimator(model, 'wobn-ckpts', config=tf.estimator.RunConfig(save_summary_steps=2),\n",
    "                              params=hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().system_raw(\"start tensorboard --logdir wobn-ckpts\") #Windows\n",
    "# get_ipython().system_raw(\"tensorboard --logdir wobnd-ckpts &\") #Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.3997, 'loss': 1.6813979, 'global_step': 20}\n",
      "{'accuracy': 0.4834, 'loss': 1.436868, 'global_step': 40}\n",
      "{'accuracy': 0.5223, 'loss': 1.3160748, 'global_step': 60}\n",
      "{'accuracy': 0.563, 'loss': 1.2359521, 'global_step': 80}\n",
      "{'accuracy': 0.5723, 'loss': 1.1918161, 'global_step': 100}\n",
      "{'accuracy': 0.5985, 'loss': 1.1358802, 'global_step': 120}\n",
      "{'accuracy': 0.5969, 'loss': 1.1330194, 'global_step': 140}\n",
      "{'accuracy': 0.6238, 'loss': 1.0888523, 'global_step': 160}\n",
      "{'accuracy': 0.6115, 'loss': 1.1035516, 'global_step': 180}\n",
      "{'accuracy': 0.6409, 'loss': 1.0914321, 'global_step': 200}\n",
      "{'accuracy': 0.6447, 'loss': 1.0573525, 'global_step': 220}\n",
      "{'accuracy': 0.6415, 'loss': 1.1172442, 'global_step': 240}\n",
      "{'accuracy': 0.6298, 'loss': 1.1756989, 'global_step': 260}\n",
      "{'accuracy': 0.6507, 'loss': 1.1496994, 'global_step': 280}\n",
      "{'accuracy': 0.6569, 'loss': 1.2456324, 'global_step': 300}\n",
      "{'accuracy': 0.5878, 'loss': 1.4401076, 'global_step': 320}\n",
      "{'accuracy': 0.6286, 'loss': 1.2964361, 'global_step': 340}\n",
      "{'accuracy': 0.6269, 'loss': 1.4488766, 'global_step': 360}\n",
      "{'accuracy': 0.6193, 'loss': 1.5120075, 'global_step': 380}\n",
      "{'accuracy': 0.6428, 'loss': 1.4297953, 'global_step': 400}\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    wobn.train(inp_fn)\n",
    "    print(wobn.evaluate(test_inp_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams['with_bn'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wbn = tf.estimator.Estimator(model, 'wbn-ckpts', config=tf.estimator.RunConfig(save_summary_steps=2),\n",
    "                             params=hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().system_raw(\"start tensorboard --logdir wbn-ckpts --port 6007\") #Windows\n",
    "# get_ipython().system_raw(\"tensorboard --logdir wobnd-ckpts &\") #Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.1, 'loss': 2.5260954, 'global_step': 20}\n",
      "{'accuracy': 0.1, 'loss': 2.61592, 'global_step': 40}\n",
      "{'accuracy': 0.1, 'loss': 2.9976683, 'global_step': 60}\n",
      "{'accuracy': 0.1, 'loss': 3.0407357, 'global_step': 80}\n",
      "{'accuracy': 0.1177, 'loss': 3.8774574, 'global_step': 100}\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    wbn.train(inp_fn)\n",
    "    print(wbn.evaluate(test_inp_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "#### Without BN\n",
    "![Graph (Without BN)](images/wobn.png)\n",
    "#### With BN\n",
    "![Graph (with BN)](images/wbn.png)\n",
    "\n",
    "As we can see, the loss converges much faster in the version using Batch Normalization. Version without BN takes about 350 steps to reach 0.2 loss, whereas version with BN does it in just 70. Also, we can see that the loss reduction is gradual. There are no ups and downs. This is due to the reduction of internal covariant shift for each layer as done by BN.\n",
    "\n",
    "#### Overfitting\n",
    "Here, we can see that the eval losses goes up during training - an obvious indication of overfitting. For now, we have concentrated only to know how the BN fastens up the regular networks. So, I didn't use any regularization.\n",
    "\n",
    "BN also provides a weak form of regularization. But as we can see, it cannot prevent overfitting. It should be used with any other regularization layers like dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about Batch Normalization in the [original paper](https://arxiv.org/abs/1502.03167). You can try running this notebook in your local system or in [Google Colab](https://drive.google.com/file/d/1w7k4r7CQaqnK_f6596SbuON9xcaw5lNo/view?usp=sharing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
